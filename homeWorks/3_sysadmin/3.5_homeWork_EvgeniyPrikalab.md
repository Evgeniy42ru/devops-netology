# Домашнее задание к занятию ["3.5. Файловые системы"](https://github.com/netology-code/sysadm-homeworks/tree/devsys10/03-sysadmin-05-fs)

1. Узнайте о [sparse](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D0%B7%D1%80%D0%B5%D0%B6%D1%91%D0%BD%D0%BD%D1%8B%D0%B9_%D1%84%D0%B0%D0%B9%D0%BB) (разряженных) файлах.          
Ответ:
```
Разрежённый файл - вместо реальной записи нулевых байт на диск, производится запись иноформации о них в файл. При работе с файлом чтение информации о нулевых байтах также идёт из файла а не с диска.
```

2. Могут ли файлы, являющиеся жесткой ссылкой на один объект, иметь разные права доступа и владельца? Почему?   
Ответ:
```
Нет. Хардлинки имеют идентичные права доступов и владельцев. При изменении прав одного хардлинка изменения прав применяются ко всем.

Предполагаю ради безопасности, при ограничении доступов мы хотим чтобы ограничения действовали на объект, из какой ссылки он бы ни вызывался.

Нельзя создать хардлинк из под рандомного пользователя с произвольными правами доступа. Всегда учитываются существующие права.

```

3. Сделайте vagrant destroy на имеющийся инстанс Ubuntu. Замените содержимое Vagrantfile следующим:
```
Vagrant.configure("2") do |config|
  config.vm.box = "bento/ubuntu-20.04"
  config.vm.provider :virtualbox do |vb|
    lvm_experiments_disk0_path = "/tmp/lvm_experiments_disk0.vmdk"
    lvm_experiments_disk1_path = "/tmp/lvm_experiments_disk1.vmdk"
    vb.customize ['createmedium', '--filename', lvm_experiments_disk0_path, '--size', 2560]
    vb.customize ['createmedium', '--filename', lvm_experiments_disk1_path, '--size', 2560]
    vb.customize ['storageattach', :id, '--storagectl', 'SATA Controller', '--port', 1, '--device', 0, '--type', 'hdd', '--medium', lvm_experiments_disk0_path]
    vb.customize ['storageattach', :id, '--storagectl', 'SATA Controller', '--port', 2, '--device', 0, '--type', 'hdd', '--medium', lvm_experiments_disk1_path]
  end
end
```
vagrant destroy
```
evgeniy@r2-d2:~/projects/devops-netology/vagrant$ ./vagrant destroy
    default: Are you sure you want to destroy the 'default' VM? [y/N] y
==> default: Destroying VM and associated drives...
```
Смотрим что получилось:         
vagrant up      
vagrant ssh     
sudo fdisk -l
```
Disk /dev/sda: 64 GiB, 68719476736 bytes, 134217728 sectors
Disk model: VBOX HARDDISK   
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: gpt
Disk identifier: B4F1CD46-1589-455C-BA21-5171874A019C

Device       Start       End   Sectors Size Type
/dev/sda1     2048      4095      2048   1M BIOS boot
/dev/sda2     4096   2101247   2097152   1G Linux filesystem
/dev/sda3  2101248 134215679 132114432  63G Linux filesystem


Disk /dev/sdb: 2.51 GiB, 2684354560 bytes, 5242880 sectors
Disk model: VBOX HARDDISK   
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes


Disk /dev/sdc: 2.51 GiB, 2684354560 bytes, 5242880 sectors
Disk model: VBOX HARDDISK   
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
```

4. Используя fdisk, разбейте первый диск на 2 раздела: 2 Гб, оставшееся пространство.           
Ответ:
```
vagrant@vagrant:~$ sudo fdisk /dev/sdb 

Welcome to fdisk (util-linux 2.34).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.

Device does not contain a recognized partition table.
Created a new DOS disklabel with disk identifier 0x6727dd19.

Command (m for help): m

Help:

  DOS (MBR)
   a   toggle a bootable flag
   b   edit nested BSD disklabel
   c   toggle the dos compatibility flag

  Generic
   d   delete a partition
   F   list free unpartitioned space
   l   list known partition types
   n   add a new partition
   p   print the partition table
   t   change a partition type
   v   verify the partition table
   i   print information about a partition

  Misc
   m   print this menu
   u   change display/entry units
   x   extra functionality (experts only)

  Script
   I   load disk layout from sfdisk script file
   O   dump disk layout to sfdisk script file

  Save & Exit
   w   write table to disk and exit
   q   quit without saving changes

  Create a new label
   g   create a new empty GPT partition table
   G   create a new empty SGI (IRIX) partition table
   o   create a new empty DOS partition table
   s   create a new empty Sun partition table


Command (m for help): g
Created a new GPT disklabel (GUID: EEDE4DAE-275F-454D-B1E1-1B5F6DD724F7).

Command (m for help): n
Partition number (1-128, default 1): 1
First sector (2048-5242846, default 2048): 2048
Last sector, +/-sectors or +/-size{K,M,G,T,P} (2048-5242846, default 5242846): +2G  

Created a new partition 1 of type 'Linux filesystem' and of size 2 GiB.

Command (m for help): n
Partition number (2-128, default 2): 2
First sector (4196352-5242846, default 4196352): 
Last sector, +/-sectors or +/-size{K,M,G,T,P} (4196352-5242846, default 5242846): 

Created a new partition 2 of type 'Linux filesystem' and of size 511 MiB.

Command (m for help): w
The partition table has been altered.
Calling ioctl() to re-read partition table.
Syncing disks.
```
Проверяем:      
sudo fdisk -l
```
Disk /dev/sdb: 2.51 GiB, 2684354560 bytes, 5242880 sectors
Disk model: VBOX HARDDISK   
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: gpt
Disk identifier: EEDE4DAE-275F-454D-B1E1-1B5F6DD724F7

Device       Start     End Sectors  Size Type
/dev/sdb1     2048 4196351 4194304    2G Linux filesystem
/dev/sdb2  4196352 5242846 1046495  511M Linux filesystem
```

5. Используя sfdisk, перенесите данную таблицу разделов на второй диск.         
Ответ:          
```
vagrant@vagrant:~$ sudo sfdisk -d /dev/sdb | sudo sfdisk /dev/sdc
Checking that no-one is using this disk right now ... OK

Disk /dev/sdc: 2.51 GiB, 2684354560 bytes, 5242880 sectors
Disk model: VBOX HARDDISK   
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes

>>> Script header accepted.
>>> Script header accepted.
>>> Script header accepted.
>>> Script header accepted.
>>> Script header accepted.
>>> Script header accepted.
>>> Created a new GPT disklabel (GUID: EEDE4DAE-275F-454D-B1E1-1B5F6DD724F7).
/dev/sdc1: Created a new partition 1 of type 'Linux filesystem' and of size 2 GiB.
/dev/sdc2: Created a new partition 2 of type 'Linux filesystem' and of size 511 MiB.
/dev/sdc3: Done.

New situation:
Disklabel type: gpt
Disk identifier: EEDE4DAE-275F-454D-B1E1-1B5F6DD724F7

Device       Start     End Sectors  Size Type
/dev/sdc1     2048 4196351 4194304    2G Linux filesystem
/dev/sdc2  4196352 5242846 1046495  511M Linux filesystem

The partition table has been altered.
Calling ioctl() to re-read partition table.
Syncing disks.
vagrant@vagrant:~$ 
```
Проверяем:      
sudo fdisk -l
```
Disk /dev/sdb: 2.51 GiB, 2684354560 bytes, 5242880 sectors
Disk model: VBOX HARDDISK   
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: gpt
Disk identifier: EEDE4DAE-275F-454D-B1E1-1B5F6DD724F7

Device       Start     End Sectors  Size Type
/dev/sdb1     2048 4196351 4194304    2G Linux filesystem
/dev/sdb2  4196352 5242846 1046495  511M Linux filesystem


Disk /dev/sdc: 2.51 GiB, 2684354560 bytes, 5242880 sectors
Disk model: VBOX HARDDISK   
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: gpt
Disk identifier: EEDE4DAE-275F-454D-B1E1-1B5F6DD724F7

Device       Start     End Sectors  Size Type
/dev/sdc1     2048 4196351 4194304    2G Linux filesystem
/dev/sdc2  4196352 5242846 1046495  511M Linux filesystem
```

6. Соберите mdadm RAID1 на паре разделов 2 Гб.  
Ответ:          
Собираем RAID 1
```
vagrant@vagrant:~$ sudo mdadm --create --verbose /dev/md0 -l 1 -n 2 /dev/sd{b1,c1}
mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store '/boot' on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
mdadm: size set to 2094080K
Continue creating array? y
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
```

7. Соберите mdadm RAID0 на второй паре маленьких разделов.      
Ответ:          
Собираем RAID 0 
```
vagrant@vagrant:~$ sudo mdadm --create --verbose /dev/md1 -l 0 -n 2 /dev/sd{b2,c2}
mdadm: chunk size defaults to 512K
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md1 started.
vagrant@vagrant:~$
```
Проверяем:
```
vagrant@vagrant:~$ lsblk
NAME                      MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
sdb                         8:16   0  2.5G  0 disk  
├─sdb1                      8:17   0    2G  0 part  
│ └─md0                     9:0    0    2G  0 raid1 
└─sdb2                      8:18   0  511M  0 part  
  └─md1                     9:1    0 1017M  0 raid0 
sdc                         8:32   0  2.5G  0 disk  
├─sdc1                      8:33   0    2G  0 part  
│ └─md0                     9:0    0    2G  0 raid1 
└─sdc2                      8:34   0  511M  0 part  
  └─md1                     9:1    0 1017M  0 raid0 
```

8. Создайте 2 независимых PV на получившихся md-устройствах.    
Ответ:          
```
vagrant@vagrant:~$ sudo pvcreate /dev/md0
  Physical volume "/dev/md0" successfully created.
vagrant@vagrant:~$ sudo pvcreate /dev/md1
  Physical volume "/dev/md1" successfully created.
vagrant@vagrant:~$ 
```
Проверяем
```
vagrant@vagrant:~$ sudo pvdisplay
  --- Physical volume ---
  PV Name               /dev/sda3
  VG Name               ubuntu-vg
  PV Size               <63.00 GiB / not usable 0   
  Allocatable           yes 
  PE Size               4.00 MiB
  Total PE              16127
  Free PE               8063
  Allocated PE          8064
  PV UUID               sDUvKe-EtCc-gKuY-ZXTD-1B1d-eh9Q-XldxLf
   
  "/dev/md0" is a new physical volume of "<2.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/md0
  VG Name               
  PV Size               <2.00 GiB
  Allocatable           NO
  PE Size               0   
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               fPqL6g-khE2-WTwi-PPuT-q9Oc-CynX-mjb8lc
   
  "/dev/md1" is a new physical volume of "1017.00 MiB"
  --- NEW Physical volume ---
  PV Name               /dev/md1
  VG Name               
  PV Size               1017.00 MiB
  Allocatable           NO
  PE Size               0   
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               6f5p5b-jsHE-Os0q-nYq8-lE74-PYc6-6cJaJy
```

9. Создайте общую volume-group на этих двух PV. 
Ответ:
```
vagrant@vagrant:~$ sudo vgcreate raid_volume_group /dev/md0 /dev/md1
  Volume group "raid_volume_group" successfully created
```
Проверяем
```
vagrant@vagrant:~$ sudo vgdisplay
  --- Volume group ---
  VG Name               ubuntu-vg
  System ID             
  Format                lvm2
  Metadata Areas        1
  Metadata Sequence No  2
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                1
  Open LV               1
  Max PV                0
  Cur PV                1
  Act PV                1
  VG Size               <63.00 GiB
  PE Size               4.00 MiB
  Total PE              16127
  Alloc PE / Size       8064 / 31.50 GiB
  Free  PE / Size       8063 / <31.50 GiB
  VG UUID               aK7Bd1-JPle-i0h7-5jJa-M60v-WwMk-PFByJ7
   
  --- Volume group ---
  VG Name               raid_volume_group
  System ID             
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               <2.99 GiB
  PE Size               4.00 MiB
  Total PE              765
  Alloc PE / Size       0 / 0   
  Free  PE / Size       765 / <2.99 GiB
  VG UUID               3dU34o-1DLJ-mp3L-t6ar-Z40V-0qDg-8JZmhf
   
vagrant@vagrant:~$
```

10. Создайте LV размером 100 Мб, указав его расположение на PV с RAID0.         
Ответ:
```
vagrant@vagrant:~$ sudo lvcreate --size=100MB raid_volume_group /dev/md1
  Logical volume "lvol0" created.
vagrant@vagrant:~$
```
Проверяем
```
vagrant@vagrant:~$ sudo lvdisplay
  --- Logical volume ---
  LV Path                /dev/ubuntu-vg/ubuntu-lv
  LV Name                ubuntu-lv
  VG Name                ubuntu-vg
  LV UUID                ftN15m-3lML-YH5x-R5P2-kLCd-kzW3-32dlqO
  LV Write Access        read/write
  LV Creation host, time ubuntu-server, 2021-12-19 19:37:44 +0000
  LV Status              available
  # open                 1
  LV Size                31.50 GiB
  Current LE             8064
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0
   
  --- Logical volume ---
  LV Path                /dev/raid_volume_group/lvol0
  LV Name                lvol0
  VG Name                raid_volume_group
  LV UUID                jsEasO-oTtg-PAfJ-nJ4v-0eCC-Y07i-v1lQOY
  LV Write Access        read/write
  LV Creation host, time vagrant, 2022-05-02 11:02:17 +0000
  LV Status              available
  # open                 0
  LV Size                100.00 MiB
  Current LE             25
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     4096
  Block device           253:1
   
vagrant@vagrant:~$
```

11. Создайте mkfs.ext4 ФС на получившемся LV.   
Ответ:
```
vagrant@vagrant:~$ sudo mkfs.ext4 /dev/raid_volume_group/lvol0
mke2fs 1.45.5 (07-Jan-2020)
Creating filesystem with 25600 4k blocks and 25600 inodes

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (1024 blocks): done
Writing superblocks and filesystem accounting information: done
```

12. Смонтируйте этот раздел в любую директорию, например, /tmp/new.             
Ответ:
```
vagrant@vagrant:~$ mkdir /tmp/raid_test
vagrant@vagrant:~$ sudo mount /dev/raid_volume_group/lvol0 /tmp/raid_test/
vagrant@vagrant:~$ ls /tmp/raid_test/
lost+found
vagrant@vagrant:~$ cd /tmp/raid_test/
vagrant@vagrant:/tmp/raid_test$
```

13. Поместите туда тестовый файл, например wget https://mirror.yandex.ru/ubuntu/ls-lR.gz -O /tmp/new/test.gz.   
Ответ:
```
vagrant@vagrant:/tmp/raid_test$ sudo wget https://mirror.yandex.ru/ubuntu/ls-lR.gz -O /tmp/raid_test/test.gz
--2022-05-02 11:09:48--  https://mirror.yandex.ru/ubuntu/ls-lR.gz
Resolving mirror.yandex.ru (mirror.yandex.ru)... 213.180.204.183, 2a02:6b8::183
Connecting to mirror.yandex.ru (mirror.yandex.ru)|213.180.204.183|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 22874551 (22M) [application/octet-stream]
Saving to: ‘/tmp/raid_test/test.gz’

/tmp/raid_test/test.gz       100%[============================================>]  21.81M  5.08MB/s    in 4.4s    

2022-05-02 11:09:53 (4.96 MB/s) - ‘/tmp/raid_test/test.gz’ saved [22874551/22874551]

vagrant@vagrant:/tmp/raid_test$
```

14. Прикрепите вывод lsblk.     
Ответ:
```
vagrant@vagrant:/tmp/raid_test$ lsblk
NAME                          MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
loop0                           7:0    0 55.4M  1 loop  /snap/core18/2128
loop2                           7:2    0 70.3M  1 loop  /snap/lxd/21029
loop3                           7:3    0 44.7M  1 loop  /snap/snapd/15534
loop4                           7:4    0 55.5M  1 loop  /snap/core18/2344
loop5                           7:5    0 61.9M  1 loop  /snap/core20/1434
loop6                           7:6    0 67.8M  1 loop  /snap/lxd/22753
sda                             8:0    0   64G  0 disk  
├─sda1                          8:1    0    1M  0 part  
├─sda2                          8:2    0    1G  0 part  /boot
└─sda3                          8:3    0   63G  0 part  
  └─ubuntu--vg-ubuntu--lv     253:0    0 31.5G  0 lvm   /
sdb                             8:16   0  2.5G  0 disk  
├─sdb1                          8:17   0    2G  0 part  
│ └─md0                         9:0    0    2G  0 raid1 
└─sdb2                          8:18   0  511M  0 part  
  └─md1                         9:1    0 1017M  0 raid0 
    └─raid_volume_group-lvol0 253:1    0  100M  0 lvm   /tmp/raid_test
sdc                             8:32   0  2.5G  0 disk  
├─sdc1                          8:33   0    2G  0 part  
│ └─md0                         9:0    0    2G  0 raid1 
└─sdc2                          8:34   0  511M  0 part  
  └─md1                         9:1    0 1017M  0 raid0 
    └─raid_volume_group-lvol0 253:1    0  100M  0 lvm   /tmp/raid_test
vagrant@vagrant:/tmp/raid_test$
```

15. Протестируйте целостность файла:            
Ответ:
```
vagrant@vagrant:/tmp/raid_test$ gzip -t /tmp/raid_test/test.gz 
vagrant@vagrant:/tmp/raid_test$ echo $?
0
vagrant@vagrant:/tmp/raid_test$
```

16. Используя pvmove, переместите содержимое PV с RAID0 на RAID1.               
Ответ:
```
vagrant@vagrant:/tmp/raid_test$ sudo pvmove /dev/md1 /dev/md0
  /dev/md1: Moved: 12.00%
  /dev/md1: Moved: 100.00%
vagrant@vagrant:/tmp/raid_test$
```

17. Сделайте --fail на устройство в вашем RAID1 md.             
Ответ:
```
vagrant@vagrant:/tmp/raid_test$ sudo mdadm --fail /dev/md0 /dev/sdb1
mdadm: set /dev/sdb1 faulty in /dev/md0
vagrant@vagrant:/tmp/raid_test$
```

18. Подтвердите выводом dmesg, что RAID1 работает в деградированном состоянии.
Ответ:
```
vagrant@vagrant:/tmp/raid_test$ dmesg | tail -2
[ 2303.548977] md/raid1:md0: Disk failure on sdb1, disabling device.
               md/raid1:md0: Operation continuing on 1 devices.
```

19. Протестируйте целостность файла, несмотря на "сбойный" диск он должен продолжать быть доступен:
```
vagrant@vagrant:/tmp/raid_test$ gzip -t /tmp/raid_test/test.gz 
vagrant@vagrant:/tmp/raid_test$ echo $?
0
vagrant@vagrant:/tmp/raid_test$
```

20. Погасите тестовый хост, vagrant destroy.
```
evgeniy@r2-d2:~/projects/devops-netology/vagrant$ ./vagrant destroy
    default: Are you sure you want to destroy the 'default' VM? [y/N] y
==> default: Forcing shutdown of VM...
==> default: Destroying VM and associated drives...
evgeniy@r2-d2:~/projects/devops-netology/vagrant$
```